# ============================================================================
# NexusCore LLM Server - Environment Configuration Template
# ============================================================================
# Copy this file to .env.local in the project root and fill in your values

# ----------------------------------------------------------------------------
# REQUIRED: llama.cpp Server Path
# ----------------------------------------------------------------------------
LLAMA_SERVER_PATH=llama-server

# If llama-server is not in PATH:
# LLAMA_SERVER_PATH=/path/to/llama.cpp/llama-server

# ----------------------------------------------------------------------------
# REQUIRED: Model Storage Directory
# ----------------------------------------------------------------------------
MODEL_PATH=./models

# Example with custom path:
# MODEL_PATH=/mnt/nvme/models/gguf

# ----------------------------------------------------------------------------
# Server Ports
# ----------------------------------------------------------------------------
PORT=8080
VITE_API_URL=http://localhost:8080/api

# ----------------------------------------------------------------------------
# OPTIONAL: Gemini AI for Log Analysis
# ----------------------------------------------------------------------------
VITE_GEMINI_API_KEY=

# Get key from: https://makersuite.google.com/app/apikey
# Example: VITE_GEMINI_API_KEY=AIzaSyD...your_key_here

# ============================================================================
# Setup Instructions:
# 1. Copy this file: cp env.example.txt .env.local
# 2. Edit .env.local with your values
# 3. Ensure llama.cpp is built with CUDA: make LLAMA_CUBLAS=1
# 4. Add .gguf model files to ./models directory
# 5. Run: npm run dev:all
# ============================================================================

